{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outro\n",
    "\n",
    "Michael Littman & Charles Isbell\n",
    "\n",
    "### Deep Neural Networks, Deep Learning \n",
    "new techniques for getting signal through multiple layers\n",
    "\n",
    "It was not clear that you could do better with multiple layers.  New set of techniques these days to make it come back in style.\n",
    "\n",
    "### Big Data\n",
    "Issues that come up when you have tons of data.\n",
    "\n",
    "Algorithmic challenges from gigantic datasets. Linear too slow.\n",
    "\n",
    "Computation has changes how science has been done.\n",
    "\n",
    "### Semi-supervised Learning\n",
    "Imagine you have tons of webpages with info on cities, you label some of them.  You can structure enough of it with unsupervised learning that will let you then label them using supervised learning.\n",
    "\n",
    "### The Assumption of Balanced Labels - Supervised\n",
    "Assumption that half of the data set is positive and half is negative which is wrong.\n",
    "\n",
    "For example, trying to find a terrorist, most people aren't.\n",
    "\n",
    "The cost of being wrong in this situation is high.  We can redo how we do error.  Another one is cascade learning by Paul Viola and Mike Jones.\n",
    "\n",
    "Build a learner that labels half positive half negative.  All the negative ones are actual true negative, and the positive will have many false positives and all the positives.  They COULD be threats.  We repeat this process over and over again similar to **boosting** until it is actually 50/50.\n",
    "\n",
    "Notice as we get smaller and smaller and smaller, we can put 2x as much computational energy after each level by a factor of 2.\n",
    "\n",
    "Now we have a series of learners that get more sophisticated and the last one will still do bad because of different distributions of data, so we still need these cascade of learners.\n",
    "\n",
    "Finding face.\n",
    "\n",
    "### Unsupervised Learning\n",
    "tf-idf: term frequency inverse document frequency\n",
    "\n",
    "What similarity equations do you use to see how close queries are?\n",
    "\n",
    "The amount of weight you put in the appearance in a document should be how often it comes up.  For example, if snow comes up a lot in a text, we can classify it as a snow.\n",
    "\n",
    "Well, how many documents have that word in the text? So the inverse document frequency is very small so do not put much weight on it if it shows up everywhere.\n",
    "\n",
    "Spectral clustering does not get caught in local maxima.\n",
    "\n",
    "Randomized optimization - cross entropy similar to MIMIC\n",
    "\n",
    "### Reinforcement Learning\n",
    "Function Approximation - Learn something about the environment with supervised learning.  Unsupervised Learning -> Feature selection.  function approximation is like feature extraction, state, action, etc etc.\n",
    "\n",
    "POMDPs - Partially Observable MDP.  The agent always has complete state information, but in POMDP, you are OBSERVING the state and making assumptions.\n",
    "\n",
    "Game Theory, Economics, Sociology, Marketing, Psychology -> Behavioral Economics, neruoeconomics.\n",
    "\n",
    "Interactive Computing. State abstraction.\n",
    "\n",
    "##### Inverse RL\n",
    "\n",
    "Reward Function interact with an environment and get a behavior\n",
    "\n",
    "Inverse RL\n",
    "behaviour -> Inverse RL -> Rewards\n",
    "\n",
    "RL learning framework as a programming framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
