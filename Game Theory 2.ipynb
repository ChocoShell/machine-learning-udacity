{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterated Prisoner's Dilemma\n",
    "\n",
    "Prisoners have a choice: defect or cooperate with each other.\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "-1,-1 & -9,0 \\\\\n",
    "0,-9 & -6,-6\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "If you know what the last round is, the prisoners will choose to defect always, but if they do not know the last round, they may not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertain End: Discounting\n",
    "\n",
    "With probability $\\gamma$, game continues.\n",
    "\n",
    "Every round could be your last. or not.\n",
    "\n",
    "Expected # rounds? Finit if $\\gamma < 1$ => $\\frac{1}{1-\\gamma}$\n",
    "\n",
    "Gamma is similar to the discount factor we discussed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tit for Tat\n",
    "\n",
    "Famous IPD strategy\n",
    "- cooperate on first round\n",
    "- copy opponent's previous move thereafter\n",
    "\n",
    "![tftstate](images/tftstate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tit for Tat: Quiz\n",
    "\n",
    "![tftquiz](images/tftquiz.PNG)\n",
    "\n",
    "### Tit for Tat: Answer\n",
    "\n",
    "![tftquizans](images/tftquizans.PNG)\n",
    "\n",
    "### Tit for Tat: Best Strategy\n",
    "\n",
    "![1/6](images/tftbeststratquiz.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Response to a Finite-State Strategy\n",
    "\n",
    "- States labeled with opponent's choice (green)\n",
    "- edges labeled with our choice (black)\n",
    "- edges annotated with our payoff for that choice (red)\n",
    "\n",
    "Our choices impacts our payoff and future decisions of the opponent.\n",
    "- the matrix was all we needed once!\n",
    "- MDP -> we can use value iteration to solve MDP.\n",
    "\n",
    "![tftfsm](images/tftfsm.PNG)\n",
    "\n",
    "Only 3 strategies: Always C, Always D, D-C-D-C..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Repsonses in IPD\n",
    "\n",
    "What is the best answer if your opponent is:\n",
    "- Always Cooperating?\n",
    "- Always Defecting?\n",
    "- TFT?\n",
    "\n",
    "Mutual best repsonse.  Pair of strategies where each best response to other. Nash! Always Defecting and TFT are the best mutual response.  No one would want to change their strategies in this equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated Games and the Folk Theorem\n",
    "\n",
    "General Idea: In repeated games, the possibility of retaliation opens the door for cooperation.\n",
    "\n",
    "What's a \"Folk Theorem\"? - oral tradition.  Something that is known just not published.\n",
    "\n",
    "In mathematics: Results known, at least to experts in the field, and considered to have established status, but not published in complete form.\n",
    "\n",
    "#### Folk Theorem\n",
    "\n",
    "In game theory, though, Folk Theorem refers to a particular result:\n",
    "\n",
    "Describes the set of payoffs that can result from Nash strategies in __repeated__ games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax quiz\n",
    "If player A tries to protect itself againt malicious strategies, how much of a reward on average can they hope to see?\n",
    "\n",
    "![minmax](images/minmaxquiz.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folk Theorem\n",
    "Any feasible payoff profile that strictly dominates the minimax(pure strats)/security level(mixed strats) profile can be realized as a Nash equilibrium payoff profile, with sufficiently large discount factor.\n",
    "\n",
    "Proof: If it strictly dominates the minmax profile, can use it as a threat. Better off doing what you are told!\n",
    "\n",
    "#### Grim Trigger\n",
    "\n",
    "![grimtrigger](images/grimtrigger.PNG)\n",
    "\n",
    "Will cooperate until you defect, then will always defect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implausible Threats\n",
    "\n",
    "**Subgame perfect**: Always best response independent of history.\n",
    "\n",
    "Is there a history you can feed these machines so that the machine does not act optimally and there is an outside action you can take to maximize reward?\n",
    "\n",
    "##### Grim vs TFT\n",
    "\n",
    "If you set the first move to TFT -> D, and Grim C.  We get into an always defect strat which is not optimal.\n",
    "\n",
    "Grim vs TFT is not subgame perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pavlov\n",
    "\n",
    "![pavlov](images/pavlov.PNG)\n",
    "\n",
    "Pavlov: cooperate if agree, defect if disagree.\n",
    "\n",
    "[Paper On Pavlov Strategy vs TFT](http://www.pnas.org/content/pnas/93/7/2686.full.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Folk Theorem\n",
    "\n",
    "2 player bimatrix(different rewards for players) -> average reward repeated with a high discount.\n",
    "\n",
    "Can build Pavlov-like machines for any game.  Construct subgame perfect Nash equilibrium for any game in polynomial time.\n",
    "\n",
    "- If mutual benefical relationship is possible, then we can build Pavlov.\n",
    "- If game is zero-sum, we can solve a linear program in poly time.\n",
    "- If at most one player can improve their behavior by taking what the other player does in a zero-sum game, we can achieve Nash equilibrium.\n",
    "\n",
    "[A Polynomial-time Nash Equilibrium Algorithm for Repeated Games](https://www.cs.utexas.edu/~pstone/Papers/bib2html/b2hd-DSS04.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Games and Multiagent RL\n",
    "\n",
    "\n",
    "- North, South, East, West, X\n",
    "- First to reach goal gets \\$100.\n",
    "- Both arrive, both win\n",
    "- semi wall (50% go through)\n",
    "- coinflip if collide\n",
    "\n",
    "![multiagent](images/multiagent.PNG)\n",
    "\n",
    "Transitions are deterministic except for the semiwalls on top of A and B.  50/50 A/B will go through wall or stay where it is.\n",
    "MDP:RL :: Stochastic Game:Multiagent RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Games (Shapley)\n",
    "S: states\n",
    "\n",
    "$A_i$: Actions for player i.\n",
    "\n",
    "T: Transitions\n",
    "\n",
    "$R_i$: Rewards for player i\n",
    "\n",
    "$\\gamma$: Discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MASquiz](images/MASquiz.PNG)\n",
    "\n",
    "Second one, only one agent affects the reward and transition function so we can say games where only one player affects the game, can be simplified down into an MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Sum Stochastic Games\n",
    "\n",
    "$$Q^*_i(s, (a,b)) = R_i(s, (a,b)) + \\gamma \\sum_{s'}T(s, (a,b), s') max_{a',b'}Q^*_i(s',(a',b'))$$\n",
    "- This assumes join actions benefit me the most! optimistic delusion\n",
    "\n",
    "$$Q^*_i(s, (a,b)) = R_i(s, (a,b)) + \\gamma \\sum_{s'}T(s, (a,b), s') minimax_{a',b'}Q^*_i(s',(a',b'))$$\n",
    "- Added minimax\n",
    "- We solve the zero-sum games in the Q values and we then try to maximize our rewards in that game with the other player.\n",
    "- Zero-sum mostly focuses on 2 player.\n",
    "\n",
    "$$<s,(a,b),(r_1,r_2),s'>: Q_i(s,(a,b)) \\leftarrow^{\\alpha}... r_i+\\gamma minimax_{a',b'}Q_i(s', (a',b'))$$\n",
    "- alpha is above arrow, dots are there to separate r and arrow\n",
    "- If we're in some state, we take some joint action pair (a,b), we're given some reward pair and new state. Q value for that state and joint action pair will be updated to be closer to the reward for player i + the discounted summarized value or V value of the new state s' with minimax(a',b')\n",
    "\n",
    "- Above equation is called minimax-Q\n",
    "\n",
    "What do we know about this?\n",
    "\n",
    "- Value iteration works\n",
    "- minimax-Q converges\n",
    "- unique solution to Q*\n",
    "- policies can be computed independently (the last Q can be computed in polynomial time.)\n",
    "- update efficient\n",
    "- Q functions sufficient to specify policy (We can turn the Q values into an actual behavior.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General-Sum Games\n",
    "Do Nash instead of Minimax\n",
    "$$Q^*_i(s, (a,b)) = R_i(s, (a,b)) + \\gamma \\sum_{s'}T(s, (a,b), s') Nash_{a',b'}Q^*_i(s',(a',b'))$$\n",
    "$$<s,(a,b),(r_1,r_2),s'>: Q_i(s,(a,b)) \\leftarrow^{\\alpha}... r_i+\\gamma Nash_{a',b'}Q_i(s', (a',b'))$$\n",
    "- Nash-Q\n",
    "\n",
    "What do we know about this?\n",
    "\n",
    "- Value iteration works (nope)\n",
    "- minimax-Q converges (nope)\n",
    "- unique solution to Q* (nope)\n",
    "- policies can be computed independently (the last Q can be computed in polynomial time.) (nope, could be incompatible)\n",
    "- update efficient (nope, P = PPAD)\n",
    "- Q functions sufficient to specify policy (We can turn the Q values into an actual behavior.) (nope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lots of Ideas\n",
    "-> repeated stochastic games (folk theorem) Imagine stochastic games are repeated then we can do folk theorem for that\n",
    "-> cheaptalk-> correlated equilibrium. Communication side channel. Lets players talk to eachother but whatever they say is nonbinding(work on this by prof of class)\n",
    "-> cognitive hierarchy -> best responses. Instead of trying to solve for a equilibrium. Each player assumes everyone else has less computational power than they do, then taking the best responses to what they believe the other players will do.\n",
    "-> side payments (coco(Cooperation Competitive)values).  (If we take this action I get reward and I will give you reward.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Have We Learned\n",
    "- Iterated PD\n",
    "- Connect IPD & RL (discontiny) repeated games\n",
    "- Folk Theorem (Nash Equilibrium) (threats)\n",
    "- subgame perfection, plausible threats\n",
    "- Computational folk theorem (Michael Littman) mooc-acceptable\n",
    "- stochastic games, generalize MDPs repeated games\n",
    "- zero sum stochastic games. minimax Q works.\n",
    "- general sum games. NashQ doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Evolution of Trust showcasing multiple strategies](http://ncase.me/trust/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
